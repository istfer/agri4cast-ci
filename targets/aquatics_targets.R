message(paste0("Running Creating Aquatics Targets at ", Sys.time()))

Sys.unsetenv("AWS_DEFAULT_REGION")
Sys.unsetenv("AWS_S3_ENDPOINT")
Sys.setenv("AWS_EC2_METADATA_DISABLED"="TRUE")

Sys.setenv(TZ = 'UTC')
## 02_generate_targets_aquatics
## Process the raw data into the target variable product
library(neonstore)
library(tidyverse)
library(lubridate)
library(contentid)
library(sparklyr)
library(sparkavro)
source('R/avro_functions.R')
source('R/data_processing.R')
# spark_install(version = '3.0')

`%!in%` <- Negate(`%in%`) # not in function

avro_file_directory <- "/home/rstudio/data/aquatic_avro"
parquet_file_directory <- "/home/rstudio/data/aquatic_parquet"
EDI_file_directory <- "/home/rstudio/data/aquatic_EDI"

readRenviron("~/.Renviron") # compatible with littler
Sys.setenv("NEONSTORE_HOME" = "/home/rstudio/data/neonstore")
Sys.getenv("NEONSTORE_DB")

#temporary aquatic repo during test of new workflow
site_data <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv")
aq_sites <- site_data |> filter(aquatics == 1) |> pull(field_site_id)

message(paste0("Running Creating Aquatics Targets at ", Sys.time()))

sites <- readr::read_csv("NEON_Field_Site_Metadata_20220412.csv") |>
  dplyr::filter(aquatics == 1)

nonwadable_rivers <- sites$field_site_id[(which(sites$field_site_subtype == "Non-wadeable River"))]
lake_sites <- sites$field_site_id[(which(sites$field_site_subtype == "Lake"))]
stream_sites <- sites$field_site_id[(which(sites$field_site_subtype == "Wadeable Stream"))]
profiling_sites <- c('CRAM', 'LIRO', 'BARC', 'TOOK')

df <-  neonstore:::neon_data(product = "DP1.20288.001",
                             # start_date = "2023-01-01",
                             # end_date = "2023-10-01",
                             type="basic")

urls <- df |>
  dplyr::filter(grepl("waq_instantaneous", name)) |>
  dplyr::pull(url)

wq_portal <- duckdbfs::open_dataset(urls, format="csv", filename = TRUE) |>
  dplyr::mutate(siteID = stringr::str_sub(filename, 77,80)) |>
  dplyr::select(siteID, startDateTime, sensorDepth,
                dissolvedOxygen,,dissolvedOxygenFinalQF,
                chlorophyll,chlorophyllFinalQF,
                chlaRelativeFluorescence, chlaRelFluoroFinalQF) %>%
  dplyr::mutate(sensorDepth = as.numeric(sensorDepth),
                dissolvedOxygen = as.numeric(dissolvedOxygen),
                chla = as.numeric(chlorophyll),
                chla_RFU = as.numeric(chlaRelativeFluorescence),
                startDateTime = as_datetime(startDateTime),
                time = as_date(startDateTime)) %>%
  # sites that are not profiling do not have accurate depths - set to 0.5
  dplyr::mutate(sensorDepth = ifelse(siteID %in% profiling_sites, sensorDepth,
                                     0.5)) |>
  dplyr::filter(sensorDepth > 0 & sensorDepth < 1) |>
  dplyr::mutate(dissolvedOxygen = ifelse(dissolvedOxygenFinalQF == 1, NA, dissolvedOxygen),
                chla = ifelse(chlorophyllFinalQF == 1, NA, chla)) |>
  dplyr::rename(site_id = siteID) |>
  dplyr::group_by(site_id, time) %>%
  dplyr::summarize(oxygen = mean(dissolvedOxygen, na.rm = TRUE),
                   chla = mean(chla, na.rm = TRUE),
                   chla_RFU = mean(chla_RFU, na.rm = TRUE), .groups = "drop") %>%
  dplyr::select(time, site_id,
                oxygen, chla, chla_RFU) %>%
  pivot_longer(cols = -c("time", "site_id"), names_to = "variable", values_to = "observation") %>%
  dplyr::filter(!((variable == "chla" & site_id %in% stream_sites) |
                    (variable == "chla_RFU" & site_id %in% stream_sites))) |>
  collect()

#====================================================#
##### low latency WQ data =======
message("# download the 24/48hr pre_release data from the Google Cloud")

# where should these files be saved?

fs::dir_create(file.path(avro_file_directory,"DP1.20288.001")) # ignores existing directories unlike dir.create()

# need to figure out which month's data are required
# what is in the NEON store db?
cur_wq_month <- wq_portal %>%
  group_by(site_id) %>%
  summarise(cur_wq_date = as.Date(max(time)),
            new_date = ceiling_date(max(time), unit = 'month'))


# Download any new files from the Google Cloud
download.neon.avro(months = cur_wq_month,
                   data_product = '20288',  # WQ data product
                   path = file.path(avro_file_directory,"DP1.20288.001"))

# Delete superseded files
# Files that have been superseded by the NEON store files can be deleted from the relavent repository
# Look in each repository to see if there are files that exceed the current maximum date of the NEON
# store data
delete.neon.parquet(months = cur_wq_month,
                    path = file.path(parquet_file_directory, "wq"),
                    data_product = '20288')

delete.neon.avro(months = cur_wq_month,
                 path = file.path(avro_file_directory,"DP1.20288.001"),
                 data_product = '20288')

# The variables (term names that should be kept)
wq_vars <- c('siteName',
             'startDate',
             'sensorDepth',
             'dissolvedOxygen',
             'dissolvedOxygenExpUncert',
             'dissolvedOxygenFinalQF',
             'chlorophyll',
             'chlorophyllExpUncert',
             'chlorophyllFinalQF',
             'chlaRelativeFluorescence',
             'chlaRelFluoroFinalQF')
columns_keep <- c('siteName', 'termName', 'startDate', 'Value', 'verticalIndex')

# Generate a list of files to be read
wq_avro_files <- list.files(path = file.path(avro_file_directory, 'DP1.20288.001'),
                            pattern = '*20288',
                            recursive = T, full.names = T)

wq_parquet_files <- list.files(path = file.path(parquet_file_directory, "wq"))

new_files <- map_lgl(wq_avro_files, function(x){
  new_file <- TRUE
  if(basename(x) %in% tools::file_path_sans_ext(wq_parquet_files)){
    new_file <- FALSE
  }
  return(new_file)
})

wq_avro_files <- wq_avro_files[which(new_files)]

if(length(wq_avro_files) > 0){
  sc <- sparklyr::spark_connect(master = "local")
  # Read in each of the files and then bind by rows
  purrr::walk(.x = wq_avro_files, ~ read.avro.wq(sc= sc,
                                                 path = .x,
                                                 columns_keep = columns_keep,
                                                 dir = file.path(parquet_file_directory, "wq")))
  spark_disconnect(sc)
}


wq_pre_release <- arrow::open_dataset(file.path(parquet_file_directory, "wq")) |>
  collect()

# Combine the avro files with the portal data
wq_full <- dplyr::bind_rows(wq_portal, wq_pre_release) %>%
  dplyr::arrange(site_id, time)

wq_full <- wq_full |>
  group_by(site_id, time, variable) |>
  summarise(observation = mean(observation, na.rm = TRUE), .groups = "drop")

#==============================#

message("##### WQ QC protocol =======")
# additional QC steps implemented (FO, 2022-07-13)
##### check 1 Gross range tests on DO and chlorophyll
# DO ranges for each sensor and each season
DO_max <- 15 # gross max
DO_min <- 2 # gross min

# chlorophyll ranges
chla_max <- 200
chla_min <- 0

# GR flag will be true if either the DO concentration or the chlorophyll are
# outside the ranges specified about

wq_cleaned <- wq_full  |>
  tidyr::pivot_wider(names_from = variable,
                     values_from = observation,
                     id_cols = c(time, site_id)) |>
  dplyr::mutate(chla = ifelse(site_id == "BARC" & (lubridate::as_date(time) >= lubridate::as_date("2021-09-21") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "BLWA" & (lubridate::as_date(time) >= lubridate::as_date("2021-09-15") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "CRAM" & (lubridate::as_date(time) >= lubridate::as_date("2022-04-01") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "FLNT" & (lubridate::as_date(time) >= lubridate::as_date("2021-11-09") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "LIRO" & (lubridate::as_date(time) >= lubridate::as_date("2022-04-01") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "PRLA" & (lubridate::as_date(time) >= lubridate::as_date("2022-04-01") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "PRPO" & (lubridate::as_date(time) >= lubridate::as_date("2022-04-01") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "SUGG" & (lubridate::as_date(time) >= lubridate::as_date("2021-09-21") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "TOMB" & (lubridate::as_date(time) >= lubridate::as_date("2021-09-21") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla),
                chla = ifelse(site_id == "TOOK" & (lubridate::as_date(time) >= lubridate::as_date("2022-04-01") &
                                                     lubridate::as_date(time) <= lubridate::as_date("2023-03-16")),
                              chla_RFU,
                              chla)) |>
  tidyr::pivot_longer(cols = -c("time", "site_id"), names_to = 'variable', values_to = 'observation') |>
  dplyr::filter(variable != 'chla_RFU') %>%
  dplyr::mutate(observation = ifelse(is.na(observation),
                                     observation, ifelse(observation >= DO_min & observation <= DO_max & variable == 'oxygen',
                                                         observation, ifelse(observation >= chla_min & observation <= chla_max & variable == 'chla', observation, NA)))) %>%
  # manual cleaning based on visual inspection
  dplyr::mutate(observation = ifelse(site_id == "MAYF" &
                                       between(time, ymd("2019-01-20"), ymd("2019-02-05")) &
                                       variable == "oxygen", NA, observation),
                observation = ifelse(site_id == "WLOU" &
                                       !between(observation, 7.5, 11) &
                                       variable == "oxygen", NA, observation),
                observation = ifelse(site_id == "BARC" &
                                       observation < 4 &
                                       variable == "oxygen", NA, observation),
                observation = ifelse(site_id == "BLDE" &
                                       between(time, ymd("2020-07-01"), ymd("2020-12-31")) &
                                       variable == "oxygen", NA, observation),
                observation = ifelse(site_id == "BIGC" &
                                       between(time, ymd("2021-10-25"), ymd("2021-10-27")) &
                                       variable == "oxygen", NA, observation),
                observation = ifelse(site_id == "REDB" &
                                       time == ymd("2022-04-28") &
                                       variable == "oxygen", NA, observation))
#===============================================#
message("#### Generate hourly temperature profiles for lake #############")
message("##### NEON portal data #####")

df <-  neonstore:::neon_data(product = "DP1.20264.001",
                             # start_date = "2023-06-01",
                             # end_date = "2023-08-01",
                             type="basic",
                             site = lake_sites)

urls <- df |>
  dplyr::filter(grepl("TSD_30_min", name)) |>
  dplyr::pull(url)


hourly_temp_profile_portal <- duckdbfs::open_dataset(urls, format="csv", filename = TRUE) |>
  dplyr::mutate(site_id = stringr::str_sub(filename, 77,80),
                verticalPosition = stringr::str_sub(filename, 153,155)) |>
  dplyr::select(startDateTime, site_id, tsdWaterTempMean, thermistorDepth, tsdWaterTempFinalQF, verticalPosition) |>
  dplyr::mutate(tsdWaterTempMean = as.numeric(tsdWaterTempMean),
                thermistorDepth = as.numeric(thermistorDepth),
                tsdWaterTempFinalQF = as.numeric(tsdWaterTempFinalQF),
                verticalPosition = as.numeric(verticalPosition)) |>
  dplyr::mutate(tsdWaterTempMean = ifelse(tsdWaterTempFinalQF == 1, NA, tsdWaterTempMean)) %>%
  dplyr::rename(depth = thermistorDepth) |>
  dplyr::mutate(date = as_date(startDateTime),
                hour = str_pad(hour(startDateTime), width = 2, side = "left", pad = "0"),
                depth = round(depth, 1)) %>% # round to the nearest 0.1 m
  dplyr::summarize(temperature = mean(tsdWaterTempMean, na.rm = TRUE),.by = c("site_id", "depth", "date", "hour")) %>%
  dplyr::select(date, hour, site_id, temperature, depth) |>
  rename(observation = temperature) |>
  mutate(variable = "temperature",
         time = as_datetime(paste0(date, " ",hour, ":00:00"))) |>
  select(-date, - hour) |>
  collect() |>
  QC.temp(range = c(-5, 40), spike = 5, by.depth = T) %>%
  mutate(data_source = 'NEON_portal')

message("##### Sonde EDI data #####")
# Only 6 lake sites available on EDI
edi_url_lake <- c("https://pasta.lternet.edu/package/data/eml/edi/1071/1/7f8aef451231d5388c98eef889332a4b",
                  "https://pasta.lternet.edu/package/data/eml/edi/1071/1/2c8893684d94b9a52394060a76cab798",
                  "https://pasta.lternet.edu/package/data/eml/edi/1071/1/770e2ab9d957991a787a2f990d5a2fad",
                  "https://pasta.lternet.edu/package/data/eml/edi/1071/1/2e52d63ba4dc2040d1e5e2d11114aa93",
                  "https://pasta.lternet.edu/package/data/eml/edi/1071/1/60df35a34bb948c0ca5e5556d129aa98",
                  "https://pasta.lternet.edu/package/data/eml/edi/1071/1/004857d60d6fe7587b112d714e0380d0")
lake_edi_profile <- c("NEON.D03.BARC.DP0.20005.001.01378.csv",
                      "NEON.D05.CRAM.DP0.20005.001.01378.csv",
                      "NEON.D05.LIRO.DP0.20005.001.01378.csv",
                      "NEON.D09.PRLA.DP0.20005.001.01378.csv",
                      "NEON.D09.PRPO.DP0.20005.001.01378.csv",
                      "NEON.D03.SUGG.DP0.20005.001.01378.csv")

fs::dir_create(EDI_file_directory) # ignores existing directories unlike dir.create()
# Download the data

for(i in 1:length(edi_url_lake)){
  if (!file.exists(file.path(EDI_file_directory,  lake_edi_profile[i]))) {
    if (!dir.exists(dirname(file.path(EDI_file_directory,
                                      lake_edi_profile[i])))) {
      dir.create(dirname(file.path(EDI_file_directory,
                                   lake_edi_profile[i])))
    }
    download.file(edi_url_lake[i], destfile = file.path(EDI_file_directory, lake_edi_profile[i]))
  }
}


# List all the files in the EDI directory
edi_data <- list.files(file.path(EDI_file_directory), full.names = T)
# Get the lake sites subset
edi_lake_files <- c(edi_data[grepl(x = edi_data, pattern= lake_sites[1])],
                    edi_data[grepl(x = edi_data, pattern= lake_sites[2])],
                    edi_data[grepl(x = edi_data, pattern= lake_sites[3])],
                    edi_data[grepl(x = edi_data, pattern= lake_sites[4])],
                    edi_data[grepl(x = edi_data, pattern= lake_sites[5])],
                    edi_data[grepl(x = edi_data, pattern= lake_sites[6])])

# Calculate the hourly average profile
hourly_temp_profile_EDI <- purrr::map_dfr(.x = edi_lake_files, ~ read.csv(file = .x)) %>%
  rename('site_id' = siteID,
         'depth' = sensorDepth,
         'observation' = waterTemp) %>%
  mutate(startDate  = lubridate::ymd_hm(startDate),
         time = lubridate::ymd_h(format(startDate, '%Y-%m-%d %H')),
         depth = round(depth, digits = 1)) %>%
  group_by(site_id, time, depth) %>%
  summarise(observation = mean(observation),.groups = "drop") %>%
  mutate(variable = "temperature") %>%
  # include first QC of data
  QC.temp(range = c(-5, 40), spike = 5, by.depth = T) %>%
  mutate(data_source = 'MS_raw')

message("##### avros data #####")
message("# Download any new files from the Google Cloud")

# need to figure out which data are required
# what is in the NEON store db?
cur_tsd_month <- hourly_temp_profile_portal %>%
  group_by(site_id) %>%
  summarise(cur_wq_date = as.Date(max(time)),
            new_date = as.Date(ceiling_date(max(time), unit = 'month')))

# Download any new files from the Google Cloud
download.neon.avro(months = cur_tsd_month,
                   data_product = '20264',  # TSD data product
                   path = file.path(avro_file_directory,"DP1.20264.001"))
# Start by deleting superseded files
# Files that have been supersed by the NEON store files can be deleted from the relevent repository
# Look in each repository to see if there are files that match the current maximum month of the NEON
# store data

delete.neon.parquet(months = cur_tsd_month,
                    path = file.path(parquet_file_directory, "tsd"),
                    data_product = '20264')

delete.neon.avro(months = cur_tsd_month,
                 path = file.path(avro_file_directory, "DP1.20264.001"),
                 data_product = '20264')

# The variables (term names that should be kept)
tsd_vars <- c('siteName',
              'startDate',
              'tsdWaterTempMean',
              'thermistorDepth',
              'tsdWaterTempExpUncert',
              'tsdWaterTempFinalQF')

columns_keep <- c('siteName', 'termName', 'startDate', 'Value', 'verticalIndex')
thermistor_depths <- readr::read_csv('thermistorDepths.csv', col_types = 'ccd')

# Generate a list of files to be read
tsd_avro_files <- paste0(avro_file_directory, '/',
                         list.files(path = avro_file_directory,
                                    pattern = '*20264',
                                    recursive = T))

lake_avro_files <- c(tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[1])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[2])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[3])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[4])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[5])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[6])],
                     tsd_avro_files[grepl(x = tsd_avro_files, pattern= lake_sites[7])])

tsd_parquet_files <- list.files(path = file.path(parquet_file_directory, "tsd"))

new_files <- map_lgl(lake_avro_files, function(x){
  new_file <- TRUE
  if(basename(x) %in% tools::file_path_sans_ext(tsd_parquet_files)){
    new_file <- FALSE
  }
  return(new_file)
})

lake_avro_files <- lake_avro_files[which(new_files)]

if(length(lake_avro_files) > 0){
  sc <- sparklyr::spark_connect(master = "local")
  message("# Read in each of the files and then bind by rows")
  hourly_temp_profile_avro <- purrr::walk(.x = lake_avro_files,
                                          ~ read.avro.tsd.profile(sc= sc,
                                                                  path = .x,
                                                                  thermistor_depths = thermistor_depths,
                                                                  columns_keep = columns_keep,
                                                                  dir = file.path(parquet_file_directory, "tsd"),
                                                                  delete_files = FALSE))
  spark_disconnect(sc)
}

# Read in the pre-release
hourly_temp_profile_prerelease <- arrow::open_dataset(file.path(parquet_file_directory, "tsd")) |>
  collect()

# Combine the three data sources
hourly_temp_profile_lakes <- bind_rows(hourly_temp_profile_portal, hourly_temp_profile_EDI, hourly_temp_profile_prerelease) %>%
  arrange(time, site_id, depth) %>%
  group_by(time, site_id, depth) %>%
  summarise(observation = mean(observation, na.rm = T), .groups = "drop") |>
  mutate(variable = "temperature") |>
  select(time, site_id, depth, variable, observation)
#======================================================#

message("#### Generate surface (< 1 m) temperature #############")
message("###### Lake temperatures #####")
# Daily surface lake temperatures generated from the hourly profiles created above

daily_temp_surface_lakes <- hourly_temp_profile_lakes %>%
  dplyr::filter(depth <= 1) %>%
  mutate(time = lubridate::as_date(time)) %>%
  group_by(site_id, time) %>%
  summarise(observation = mean(observation, na.rm = T),.groups = "drop") %>%
  mutate(variable = 'temperature')

message("##### Stream temperatures #####")


df <-  neonstore:::neon_data(product = "DP1.20053.001",
                             # start_date = "2023-06-01",
                             # end_date = "2023-08-01",
                             type="basic",
                             site = stream_sites)

urls <- df |>
  dplyr::filter(grepl("TSW_30min", name)) |>
  dplyr::pull(url)

temp_streams_portal <-
  duckdbfs::open_dataset(urls, format="csv", filename = TRUE) |>
  dplyr::mutate(site_id = stringr::str_sub(filename, 77,80),
                verticalPosition = stringr::str_sub(filename, 153,155),
                horizontalPosition = stringr::str_sub(filename, 149,151)) |>
  dplyr::filter(horizontalPosition == "101" |
                  horizontalPosition == "111" | # take upstream to match WQ data
                  (horizontalPosition == "112" & site_id == "BLUE"), # no data at BLUE upstream
                finalQF == 0) %>%
  dplyr::select(startDateTime, site_id, surfWaterTempMean, finalQF) %>%
  dplyr::mutate(time = as_date(startDateTime),
                surfWaterTempMean = as.numeric(surfWaterTempMean)) %>%
  # dplyr::group_by(time, site_id) %>%
  dplyr::summarize(temperature = mean(surfWaterTempMean, na.rm = TRUE), .by = c('time', 'site_id')) %>%
  dplyr::select(time, site_id, temperature) %>%
  rename(observation = temperature) |>
  mutate(variable = "temperature") |>
  collect()

temp_streams_portal_QC <- temp_streams_portal %>%
  QC.temp(range = c(-5, 40), spike = 7, by.depth = F)
#===========================================#
message("##### Stream temperatures2 #####")
#### avros

# need to figure out which month's data are required
# what is in the NEON store db?
cur_prt_month <- temp_streams_portal_QC %>%
  group_by(site_id) %>%
  summarise(cur_wq_date = as.Date(max(time)),
            new_date = as.Date(ceiling_date(max(time), unit = 'month')))


# what is the next month from this plus the current month? These might be the same
# new_month_prt <- unique(format(c((as.Date(max(temp_streams_portal_QC$time)) %m+% months(1)), (Sys.Date() - days(2))), "%Y-%m"))

# Download any new files from the Google Cloud
download.neon.avro(months = cur_prt_month,
                   data_product = '20053',  # PRT data product
                   path = file.path(avro_file_directory,"DP1.20053.001"))

# Start by deleting superseded files
# Files that have been supersed by the NEON store files can be deleted from the relevent repository
# Look in each repository to see if there are files that match the current maximum month of the NEON
# store data

delete.neon.parquet(months = cur_prt_month,
                    path = file.path(parquet_file_directory, "prt"),
                    data_product = '20053')

delete.neon.avro(months = cur_prt_month,
                 path = file.path(avro_file_directory, "DP1.20053.001"),
                 data_product = '20053')

# The variables (term names that should be kept)
prt_vars <- c('siteName',
              'startDate',
              'surfWaterTempMean',
              'surfWaterTempExpUncert',
              'finalQF')

columns_keep <- c('siteName', 'termName', 'startDate', 'Value', 'verticalIndex')


# Generate a list of files to be read
prt_avro_files <- paste0(avro_file_directory, '/',
                         list.files(path = avro_file_directory,
                                    pattern = '*20053',
                                    recursive = T))

prt_parquet_files <- list.files(path = file.path(parquet_file_directory, "prt"))

new_files <- map_lgl(prt_avro_files, function(x){
  new_file <- TRUE
  if(basename(x) %in% tools::file_path_sans_ext(prt_parquet_files)){
    new_file <- FALSE
  }
  return(new_file)
})

prt_avro_files <- prt_avro_files[which(new_files)]

## check for bad NEON files and remove if present
problem_files <- c('/home/rstudio/data/aquatic_avro/site=BIGC/BIGC_L0_to_L1_Surface_Water_Temperature_DP1.20053.001__2021-12-27.avro',
                   '/home/rstudio/data/aquatic_avro/site=BIGC/BIGC_L0_to_L1_Surface_Water_Temperature_DP1.20053.001__2023-02-01.avro',
                   '/home/rstudio/data/aquatic_avro/site=BIGC/BIGC_L0_to_L1_Surface_Water_Temperature_DP1.20053.001__2023-01-28.avro')

problem_files <- map_lgl(prt_avro_files, function(x){
  problem_file <- FALSE
  if(stringr::str_detect(x, "BIGC_L0_to_L1_Surface_Water_Temperature_DP1.20053.001")){
    problem_file <- TRUE
  }
  return(problem_file)
})

if(any(problem_files == TRUE)){
  prt_avro_files <- prt_avro_files[!problem_files]
  message('Problem files removed')
}


if(length(prt_avro_files > 0)){
  sc <- sparklyr::spark_connect(master = "local")
  # Read in each of the files and then bind by rows
  purrr::walk(.x = prt_avro_files, ~ read.avro.prt(sc= sc,
                                                   path = .x,
                                                   columns_keep = columns_keep,
                                                   dir = file.path(parquet_file_directory, "prt")))
  spark_disconnect(sc)
}


temp_streams_prerelease <- arrow::open_dataset(file.path(parquet_file_directory, "prt")) |>
  collect()

#===============================================#

message("##### River temperature ######")
# For non-wadeable rivers need portal, EDI and avro data

df <-  neonstore:::neon_data(product = "DP1.20264.001",
                             # start_date = "2023-06-01",
                             # end_date = "2023-08-01",
                             type="basic",
                             site = nonwadable_rivers)

urls <- df |>
  dplyr::filter(grepl("TSD_30_min", name)) |>
  dplyr::pull(url)


temp_rivers_portal <- duckdbfs::open_dataset(urls, format="csv", filename = TRUE) |>
  dplyr::mutate(site_id = stringr::str_sub(filename, 77,80)) |>
  dplyr::mutate(depth = as.numeric(thermistorDepth),
                tsdWaterTempMean = as.numeric(tsdWaterTempMean),
                tsdWaterTempFinalQF = as.numeric(tsdWaterTempFinalQF)) %>%
  dplyr::select(startDateTime, site_id, tsdWaterTempMean, depth, tsdWaterTempFinalQF) %>%
  dplyr::filter(tsdWaterTempFinalQF == 0) %>%
  dplyr::mutate(time = as_date(startDateTime)) %>%
  dplyr::summarize(temperature = mean(tsdWaterTempMean, na.rm = TRUE), .by = c("time", "site_id")) %>%
  dplyr::select(time, site_id, temperature) %>%
  rename(observation = temperature) |>
  mutate(variable = "temperature") |>
  collect()

temp_rivers_portal_QC <- temp_rivers_portal %>%
  QC.temp(range = c(-5, 40), spike = 7, by.depth = F)

# EDI data
edi_url_river <- c("https://pasta.lternet.edu/package/data/eml/edi/1185/1/fb9cf9ba62ee8e8cf94cb020175e9165",
                   "https://pasta.lternet.edu/package/data/eml/edi/1185/1/fac068cff680ae28473c3e13dc75aa9f",
                   "https://pasta.lternet.edu/package/data/eml/edi/1185/1/5567ad7252b598ee40f5653e7b732ff4" )

river_edi_profile <- c("NEON.D03.FLNT.DP0.20005.001.01378.csv",
                       "NEON.D03.BLWA.DP0.20005.001.01378.csv",
                       "NEON.D03.TOMB.DP0.20005.001.01378.csv")

for(i in 1:length(edi_url_river)){
  if (!file.exists(file.path(EDI_file_directory,river_edi_profile[i]))) {
    if (!dir.exists(dirname(file.path(EDI_file_directory,
                                      river_edi_profile[i])))) {
      dir.create(dirname(file.path(EDI_file_directory,
                                   river_edi_profile[i])))
    }
    download.file(edi_url_river[i], destfile = file.path(EDI_file_directory, river_edi_profile[i]))
  }
}

edi_data <- list.files(file.path(EDI_file_directory), full.names = T)

edi_rivers <- c(edi_data[grepl(x = edi_data, pattern= nonwadable_rivers[1])],
                edi_data[grepl(x = edi_data, pattern= nonwadable_rivers[2])],
                edi_data[grepl(x = edi_data, pattern= nonwadable_rivers[3])])

# The hourly data set is for the whole water column.
temp_rivers_EDI <- purrr::map_dfr(.x = edi_rivers, ~ read.csv(file = .x)) %>%
  rename('site_id' = siteID,
         'observation' = waterTemp) %>%
  mutate(startDate  = lubridate::ymd_hm(startDate),
         time = as.Date(startDate)) %>%
  group_by(site_id, time) %>%
  summarise(observation = mean(observation),.groups = "drop") %>%
  # include first QC of data
  QC.temp(range = c(-5, 40), spike = 5, by.depth = F) |>
  mutate(variable = "temperature")


message('download non-wadable rivers avros')
cur_tsd_month <- temp_rivers_portal_QC %>%
  group_by(site_id) %>%
  summarise(cur_wq_date = as.Date(max(time)),
            new_date = as.Date(ceiling_date(max(time), unit = 'month')))

# Download any new files from the Google Cloud
download.neon.avro(months = cur_tsd_month,
                   data_product = '20264',  # TSD data product
                   path = file.path(avro_file_directory, "DP1.20264.001"))

message("Generate a list of nonwadable_rivers avro files to be read")
tsd_avro_files <- paste0(avro_file_directory, '/',
                         list.files(path = avro_file_directory,
                                    pattern = '*20264',
                                    recursive = T))

river_avro_files <- c(tsd_avro_files[grepl(x = tsd_avro_files, pattern= nonwadable_rivers[1])],
                      tsd_avro_files[grepl(x = tsd_avro_files, pattern= nonwadable_rivers[2])],
                      tsd_avro_files[grepl(x = tsd_avro_files, pattern= nonwadable_rivers[3])])

tsd_parquet_files <- list.files(path = file.path(parquet_file_directory, "river_tsd"))

new_files <- map_lgl(river_avro_files, function(x){
  new_file <- TRUE
  if(basename(x) %in% tools::file_path_sans_ext(tsd_parquet_files)){
    new_file <- FALSE
  }
  return(new_file)
})

river_avro_files <- river_avro_files[which(new_files)]


if(length(river_avro_files) > 0){
  sc <- sparklyr::spark_connect(master = "local")
  # Read in each of the files and then bind by rows
  purrr::walk(.x = river_avro_files,  ~ read.avro.tsd(sc= sc,
                                                      path = .x,
                                                      thermistor_depths = thermistor_depths,
                                                      dir = file.path(parquet_file_directory, "river_tsd"),
                                                      delete_files = FALSE))
  spark_disconnect(sc)
}


temp_rivers_prerelease <- arrow::open_dataset(file.path(parquet_file_directory, "river_tsd")) |>
  collect()

#===========================================#

message("#### surface temperatures ####")

# Combine the avro files with the portal data
temp_full <- dplyr::bind_rows(# Lakes surface temperature
  daily_temp_surface_lakes,

  # Stream temperature data
  temp_streams_portal_QC,
  temp_streams_prerelease,

  # River temperature data
  temp_rivers_portal_QC,
  temp_rivers_EDI,
  temp_rivers_prerelease) %>%
  dplyr::arrange(site_id, time) %>%
  group_by(site_id, time) %>%
  summarise(observation = mean(observation, na.rm = T),.groups = "drop") %>%
  mutate(variable = 'temperature')


#### Temp QC protocol=================

# additional QC steps implemented (FO, 2022-07-13)
##### check 1 Gross range tests on temperature
# temperature ranges
T_max <- 32 # gross max
T_min <- -2 # gross min

# GR flag will be true if the temperature is outside the range specified
temp_cleaned <-  temp_full %>%
  dplyr::mutate(observation =ifelse(observation >= T_min & observation <= T_max ,
                                    observation, NA))  %>%
  # manual cleaning based on observation
  dplyr:: mutate(observation = ifelse(site_id == "PRLA" & time <ymd("2019-01-01"),
                                      NA, observation))

#### Targets==========================
targets_long <- dplyr::bind_rows(wq_cleaned, temp_cleaned) %>%
  dplyr::arrange(site_id, time, variable) %>%
  dplyr::mutate(observation = ifelse(is.nan(observation), NA, observation))

message("#### Writing forecasts to file ####")

targets_long <- targets_long |>
  rename(datetime = time)

hourly_temp_profile_lakes <- hourly_temp_profile_lakes |>
  rename(datetime = time)

readRenviron("~/.Renviron") # compatible with littler

s3 <- arrow::s3_bucket("neon4cast-targets/aquatics",
                       endpoint_override = "data.ecoforecast.org",
                       access_key = Sys.getenv("AWS_ACCESS_KEY"),
                       secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"))

arrow::write_csv_arrow(targets_long, sink = s3$path("aquatics-targets.csv.gz"))

targets_long2 <- targets_long |>
  mutate(datetime = lubridate::as_datetime(datetime),
         duration = "P1D",
         project_id = "neon4cast") |>
  select(project_id, site_id, datetime, duration, variable, observation)

s3 <- arrow::s3_bucket("bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D",
                       endpoint_override = "sdsc.osn.xsede.org",
                       access_key = Sys.getenv("OSN_KEY"),
                       secret_key = Sys.getenv("OSN_SECRET"))

arrow::write_csv_arrow(targets_long2, sink = s3$path("aquatics-targets.csv.gz"))


s3 <- arrow::s3_bucket("neon4cast-targets/aquatics",
                       endpoint_override = "data.ecoforecast.org",
                       access_key = Sys.getenv("AWS_ACCESS_KEY"),
                       secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"))

arrow::write_csv_arrow(hourly_temp_profile_lakes, sink = s3$path("aquatics-expanded-observations.csv.gz"))

hourly_temp_profile_lakes2 <- hourly_temp_profile_lakes |>
  mutate(datetime = lubridate::as_datetime(datetime),
         duration = "PT1H",
         project_id = "neon4cast") |>
  select(project_id, site_id, datetime, duration, variable, observation)

#s3 <- arrow::s3_bucket("bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=PT1H",
#                       endpoint_override = "sdsc.osn.xsede.org",
#                       access_key = Sys.getenv("OSN_KEY"),
#                       secret_key = Sys.getenv("OSN_SECRET"))

s3 <- arrow::s3_bucket("bio230014-bucket01/challenges/",
                       endpoint_override = "sdsc.osn.xsede.org",
                       access_key = Sys.getenv("OSN_KEY"),
                       secret_key = Sys.getenv("OSN_SECRET"))

arrow::write_csv_arrow(hourly_temp_profile_lakes2, sink = s3$path("supporting_data/project_id=neon4cast/aquatics-expanded-observations.csv.gz"))

#arrow::write_csv_arrow(hourly_temp_profile_lakes2, sink = s3$path("aquatics-expanded-observations.csv.gz"))

message(paste0("Completed Aquatics Target at ", Sys.time()))
